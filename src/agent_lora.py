# src/agent_lora.py
"""
LoRA Agent Module
----------------
This module defines the UnslothAgent class, which loads a requested LoRA adapter
and performs inference to generate code based on input messages.
"""
import os
import torch
try:
    import torch._inductor.config
except ImportError:
    pass
from unsloth import FastLanguageModel
import re
import config

class UnslothAgent:
    def __init__(self):
        # 1. ç¡®å®šæ¨¡å‹è·¯å¾„
        adapter_path = str(config.ADAPTER_PATH)
        
        print(f"ğŸ§  æ­£åœ¨åŠ è½½å¾®è°ƒåçš„æ¨¡å‹: {adapter_path} ...")
        
        if not os.path.exists(adapter_path):
            print(f"âš ï¸ Adapter path not found: {adapter_path}")
            print("   âš ï¸ Loading Base Model (Qwen2.5-Coder-7B-Instruct) for Cold Start...")
            model_name = "unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit"
        else:
            model_name = adapter_path

        try:
            # 2. åŠ è½½æ¨¡å‹
            self.model, self.tokenizer = FastLanguageModel.from_pretrained(
                model_name = model_name,
                max_seq_length = 8192,
                dtype = None,
                load_in_4bit = True,
            )
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            raise e
        
        # 3. å¼€å¯æ¨ç†æ¨¡å¼ (åŠ é€Ÿ 2 å€)
        FastLanguageModel.for_inference(self.model)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def generate_code(self, messages, max_new_tokens=4096, temperature=0.7):
        """
        è¾“å…¥ï¼šæ ‡å‡†çš„ Chat æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}, ...]
        è¾“å‡ºï¼šä»£ç å­—ç¬¦ä¸²
        """
        # ä½¿ç”¨ tokenizer åº”ç”¨èŠå¤©æ¨¡æ¿
        inputs = self.tokenizer.apply_chat_template(
            messages,
            tokenize = True,
            add_generation_prompt = True, # åŠ ä¸Š <|im_start|>assistant
            return_tensors = "pt",
        ).to("cuda")

        # ç”Ÿæˆ
        outputs = self.model.generate(
            input_ids = inputs,
            max_new_tokens = max_new_tokens,
            use_cache = True,
            temperature = temperature, 
            top_p = 0.9,
            do_sample = True if temperature > 0 else False,
        )
        
        # è§£ç 
        response = self.tokenizer.batch_decode(outputs)
        response_text = response[0]
        
        # æå– Assistant çš„å›ç­”éƒ¨åˆ†
        parts = response_text.split("<|im_start|>assistant\n")
        if len(parts) > 1:
            content = parts[-1].replace("<|im_end|>", "").strip()
        else:
            content = response_text

        # æå–ä»£ç å—
        code_match = re.search(r"```python(.*?)```", content, re.DOTALL)
        if code_match:
            return code_match.group(1).strip()
        
        # æš´åŠ›æ¸…æ´—
        if "```" in content:
            return content.replace("```python", "").replace("```", "").strip()
            
        return content.strip()