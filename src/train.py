# src/train.py
"""
Training Module
--------------
This module handles the fine-tuning of the model using LoRA.
It loads the dataset, configures the model and tokenizer, and runs the training loop.
"""
import os
import torch
import torch._inductor.config
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments
import config

# --- é…ç½® ---
# è·¯å¾„é”æ­»é€»è¾‘ (ä½¿ç”¨ config æ›¿ä»£)
# output_dir moved to inside or used from config
output_dir = str(config.ADAPTER_PATH)

# æ¨¡å‹é…ç½®
max_seq_length = 2048 # å…è®¸çš„ä¸Šä¸‹æ–‡é•¿åº¦
dtype = None # è‡ªåŠ¨æ£€æµ‹ (Float16 for 3080)
load_in_4bit = True # 4bit é‡åŒ– (å…³é”®ï¼çœæ˜¾å­˜)

def train():
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹...")
    
    model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
    adapter_exists = os.path.exists(config.ADAPTER_PATH)
    if adapter_exists:
        print(f"ğŸ”„ Loading existing adapter from {config.ADAPTER_PATH} for incremental training...")
        model_name = str(config.ADAPTER_PATH)

    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = model_name,
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )

    # 2. æ·»åŠ  LoRA é€‚é…å™¨ (ç»™æ¨¡å‹åŠ å¤–æŒ‚)
    model = FastLanguageModel.get_peft_model(
        model,
        r = 16, # LoRA ç§© (è¶Šå¤§è¶Šèªæ˜ï¼Œä½†æ˜¾å­˜å ç”¨è¶Šé«˜ï¼Œ16 æ˜¯ 3080 çš„ç”œç‚¹)
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj",],
        lora_alpha = 16,
        lora_dropout = 0, 
        bias = "none", 
        use_gradient_checkpointing = "unsloth", # æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯
        random_state = 3407,
        use_rslora = False,
        loftq_config = None,
    )

    # 3. å‡†å¤‡æ•°æ®æ ¼å¼
    # Load both datasets
    print("ğŸ“¥ Loading datasets...")
    print("ğŸ“¥ Loading datasets...")
    dsl_data_file = str(config.DSL_DATA_FILE)
    primitives_data_file = str(config.PRIMITIVES_DATA_FILE)
    dream_data_file = str(config.DREAM_DATA_FILE)
    
    dataset_files = []
    
    # ç­–ç•¥ï¼šå¦‚æœ Adapter å·²å­˜åœ¨ï¼Œè¯´æ˜æ˜¯"å¤ä¹ "é˜¶æ®µï¼Œä¸å†åŠ è½½å·¨å¤§çš„åˆæˆæ•°æ®
    adapter_exists = os.path.exists(config.ADAPTER_PATH)
    
    if not adapter_exists:
        print("ğŸ‘¶ Cold Start: Loading Synthetic Data for initial training...")
        if os.path.exists(dsl_data_file):
            dataset_files.append(dsl_data_file)
        if os.path.exists(primitives_data_file):
            dataset_files.append(primitives_data_file)
    else:
        print("ğŸ“ Adapter found. Skipping synthetic data to save time (Incremental Learning).")

    if os.path.exists(dream_data_file):
        print(f"ğŸ˜´ Found Dream Data! Including {dream_data_file} in training.")
        dataset_files.append(dream_data_file)
    else:
        # å¦‚æœæ—¢æ²¡æœ‰ Adapter åˆæ²¡æœ‰ Dreamï¼Œæˆ–è€…æœ‰ Adapter ä½†æ²¡ Dream
        if not dataset_files and adapter_exists:
            print("âš ï¸ No new dreams to learn and synthetic data skipped. Exiting.")
            return

    if not dataset_files:
         raise FileNotFoundError("No training data found in data/ directory!")

    dataset = load_dataset("json", data_files=dataset_files, split="train")
    
    # Shuffle the dataset to mix task types
    dataset = dataset.shuffle(seed=42)
    
    # å®šä¹‰æ ¼å¼åŒ–å‡½æ•°ï¼šæŠŠ User/Assistant å˜æˆæ¨¡å‹èƒ½è¯»çš„ Prompt
    def formatting_prompts_func(examples):
        convos = examples["messages"]
        texts = []
        for convo in convos:
            # æå– System, User, Assistant
            system_text = convo[0]['content']
            user_text = convo[1]['content']
            assistant_text = convo[2]['content']
            
            # æ„é€  Qwen çš„ Chat æ¨¡æ¿æ ¼å¼
            text = f"<|im_start|>system\n{system_text}<|im_end|>\n<|im_start|>user\n{user_text}<|im_end|>\n<|im_start|>assistant\n{assistant_text}<|im_end|>"
            texts.append(text)
        return { "text" : texts, }

    dataset = dataset.map(formatting_prompts_func, batched = True,)

    # 4. è®¾ç½®è®­ç»ƒå‚æ•°
    trainer = SFTTrainer(
        model = model,
        tokenizer = tokenizer,
        train_dataset = dataset,
        dataset_text_field = "text",
        max_seq_length = max_seq_length,
        dataset_num_proc = 2,
        packing = False, # å¯ä»¥è®¾ä¸º True åŠ é€Ÿï¼Œä½†å®¹æ˜“çˆ†æ˜¾å­˜
        args = TrainingArguments(
            per_device_train_batch_size = 2, # 3080 æ˜¾å­˜å°ï¼Œè®¾ä¸º 2
            gradient_accumulation_steps = 4, # ç´¯ç§¯æ¢¯åº¦ï¼Œç›¸å½“äº batch_size = 8
            warmup_steps = 5,
            max_steps = 100 if adapter_exists else 500, # å¤ä¹ æ—¶æ­¥æ•°åªéœ€è¦å¾ˆå°‘ï¼Œåˆå­¦æ—¶å¤šä¸€ç‚¹
                            # æ­£å¼è®­ç»ƒå»ºè®®è®¾ä¸º 300 - 500 æ­¥
            learning_rate = 2e-4,
            fp16 = not torch.cuda.is_bf16_supported(),
            bf16 = torch.cuda.is_bf16_supported(),
            logging_steps = 1,
            optim = "adamw_8bit", # 8bit ä¼˜åŒ–å™¨ï¼Œçœæ˜¾å­˜
            weight_decay = 0.01,
            lr_scheduler_type = "linear",
            seed = 3407,
            output_dir = "outputs",
        ),
    )

    # 5. å¼€å§‹è®­ç»ƒ
    print("ğŸ”¥ æ­£åœ¨ç‚¹ç« (Training Started)...")
    trainer_stats = trainer.train()

    # 6. ä¿å­˜æ¨¡å‹ (LoRA Adapter)
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ LoRA é€‚é…å™¨åˆ°: {output_dir}")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # åŒæ—¶ä¹Ÿä¿å­˜ä¸º GGUF æ ¼å¼ (å¯é€‰ï¼Œæ–¹ä¾¿ Ollama åŠ è½½ï¼Œè¿™é‡Œå…ˆè·³è¿‡ï¼Œåç»­æ•™ä½ è½¬æ¢)
    print("âœ… è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    train()