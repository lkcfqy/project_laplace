# ğŸŒŒ Project Laplace âœ¨

Welcome to **Project Laplace**, an autonomous agent system based on **Neuro-Symbolic AI** designed to solve the **ARC (Abstraction and Reasoning Corpus)** tasks through Program Synthesis! ğŸ§©âœ¨

This project beautifully combines **Large Language Models (LLMs)**, **Monte Carlo Tree Search (MCTS)**, and **Hyperdimensional Computing (HDC)**, achieving self-evolution through a fascinating **"Wake-Sleep"** cycle mechanism. ğŸŒ—

## âœ¨ Core Features ğŸ› ï¸

* **ğŸ§  Neuro-Symbolic Architecture**: Uses a fine-tuned **Qwen2.5-Coder** to generate Python code, combined with a predefined **DSL (Domain Specific Language)** to manipulate grids, ensuring logical interpretability and precision! ğŸ¯
* **ğŸ” Wake-Sleep Guided Learning**:
  * **â˜€ï¸ Wake Phase**: Employs MCTS and the current model to attempt solving ARC tasks, generating successful trajectories known as "Dreams". â˜ï¸
  * **ğŸŒ™ Sleep Phase**: Utilizes the "dream" data (successful problem-solving trajectories) and synthetic data to perform LoRA fine-tuning on the LLM and train the value network. ğŸ’¤
* **ğŸŒ³ MCTS + HDC**: Implements a Monte Carlo Tree Search solver, utilizing **Hyperdimensional Computing (HDC)** for efficient state similarity evaluation and Neural Value Network guided search pruning. âœ‚ï¸
* **ğŸ›¡ï¸ Safe Sandbox Environment**: Features a built-in **Docker Sandbox** and AST static checks to ensure generated code runs safely in an isolated environment. ğŸ”’
* **ğŸš€ Unsloth Acceleration**: Leverages the Unsloth framework for efficient 4-bit quantized LoRA fine-tuning, drastically reducing VRAM requirements (perfect for consumer GPUs like the RTX 30 series!). ğŸ’»

## ğŸ“‚ Project Structure ğŸ“

```text
project_laplace/
â”œâ”€â”€ data/                   # Stores training data, the ARC dataset, and generated "Dreams"
â”œâ”€â”€ models/                 # Stores LoRA adapters and Value Net weights
â”œâ”€â”€ src/                    # Source code directory
â”‚   â”œâ”€â”€ agent_lora.py       # LLM Agent wrapper (Unsloth/Qwen)
â”‚   â”œâ”€â”€ bootstrap_loop.py   # Main entry for the Wake-Sleep loop
â”‚   â”œâ”€â”€ config.py           # Project path configurations
â”‚   â”œâ”€â”€ dataset_hdc.py      # Data loader for HDC training
â”‚   â”œâ”€â”€ docker_sandbox.py   # Docker sandbox for safe code execution
â”‚   â”œâ”€â”€ dsl.py              # Domain Specific Language (DSL) primitives for ARC tasks
â”‚   â”œâ”€â”€ executor.py         # Code executor (includes AST safety checks)
â”‚   â”œâ”€â”€ gen_baseline.py     # Generator for synthetic baseline data (Reverse engineering DSL)
â”‚   â”œâ”€â”€ gen_synthetic_tasks.py # Generates specific types of synthetic ARC tasks
â”‚   â”œâ”€â”€ hdc.py              # Hyperdimensional Computing (HDC) implementation
â”‚   â”œâ”€â”€ mcts.py             # Monte Carlo Tree Search solver
â”‚   â”œâ”€â”€ solve.py            # Standalone solving script (supports Sampling/Refinement/MCTS)
â”‚   â”œâ”€â”€ train.py            # LLM LoRA fine-tuning script
â”‚   â”œâ”€â”€ train_value_net.py  # Script to train the HDC Value Network
â”‚   â””â”€â”€ value_net.py        # Value Network model definition
â””â”€â”€ .gitignore

```

## ğŸ› ï¸ Installation & Environment ğŸ’»

### Prerequisites ğŸ“Œ

* Python 3.10+ ğŸ
* NVIDIA GPU (CUDA supported, 8GB+ VRAM recommended) ğŸ®
* Docker (for the safe sandbox; optional but highly recommended!) ğŸ³

### Installing Dependencies ğŸ“¦

1. **Clone the project** ğŸ“¥

```bash
git clone [https://github.com/your-username/project_laplace.git](https://github.com/your-username/project_laplace.git)
cd project_laplace

```

2. **Install Python dependencies** ğŸª„
*(Using Conda or venv is recommended)*

```bash
pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
pip install unsloth "unsloth[colab-new]"  # Or follow the official Unsloth docs
pip install docker transformers datasets trl peft bitsandbytes

```

3. **Prepare the Data** ğŸ“Š

* Ensure that the ARC dataset (`arc/training` and `arc/evaluation`) is inside the `data/` directory.
* Alternatively, run the generation scripts first to create synthetic data.

## ğŸš€ Quick Start ğŸƒâ€â™€ï¸ğŸ’¨

### 1. Generate Synthetic Data (Cold Start) ğŸ§Š

When starting with no model weights, generate some synthetic data first to teach the model the basic DSL usage!

```bash
# Generate basic DSL training samples
python src/gen_baseline.py

# Generate simple synthetic ARC tasks to test the solver
python src/gen_synthetic_tasks.py --count 50

```

### 2. Start the Wake-Sleep Loop (Recommended) ğŸŒğŸŒ›

This is the main operation mode of the project. It automatically alternates between "Task Solving" and "Model Training".

```bash
# Loops 100 times by default
python src/bootstrap_loop.py

```

### 3. Run the Solver Standalone ğŸ•µï¸â€â™€ï¸

If you just want to test the model's ability to solve specific tasks:

```bash
# Solve a specific task using MCTS mode
python src/solve.py --mode mcts --task_file data/arc/training/25d8a9c8.json

# Use Refinement mode (Self-correction)
python src/solve.py --mode refinement --steps 3 --random 5

```

### 4. Standalone Training ğŸ‹ï¸â€â™‚ï¸

To manually trigger the training process:

```bash
# Train the LLM (Qwen LoRA)
python src/train.py

# Train the HDC Value Network
python src/train_value_net.py

```

## ğŸ§  Core Logic Explained ğŸ’¡

### DSL (Domain Specific Language) ğŸ” 

Defined in `src/dsl.py`, it contains high-level abstract functions commonly used for ARC tasks, such as:

* `get_objects(grid)`: Object detection ğŸ”
* `flood_fill(grid, r, c, color)`: Flood fill ğŸŒŠ
* `move_object(grid, obj, dr, dc)`: Move object ğŸ“¦
* `detect_periodicity(grid)`: Periodicity detection ğŸ”

### MCTS Solver ğŸŒ³

Defined in `src/mcts.py`.

1. **Selection**: Uses the UCT algorithm to select the most promising nodes. âœ¨
2. **Expansion**: Calls the LLM (`UnslothAgent`) to generate Python code suggestions based on the current grid state. ğŸ“
3. **Evaluation**:
* Executes code within the sandbox. ğŸ›¡ï¸
* Uses **HDC (src/hdc.py)** to encode the grid state. ğŸ§©
* Calculates cosine similarity between the current and target states, or predicts the success rate via the **Value Net**. âš–ï¸


4. **Backprop**: Updates the path values based on the evaluation. ğŸ”™

## âš ï¸ Important Notes ğŸš¨

* **VRAM Usage**: The default configuration is optimized for 24GB VRAM. If your VRAM is smaller (e.g., 8GB-12GB), please lower the `batch_size` in `src/train.py` and ensure `load_in_4bit=True` is enabled! ğŸ“‰
* **Docker Permissions**: Running the sandbox requires the current user to have Docker permissions (on Linux, this usually means adding the user to the `docker` group). If Docker isn't used, the program will fall back to local execution, which carries security risks! âš ï¸

## ğŸ¤ Contribution ğŸ’–

Issues and Pull Requests are incredibly welcome! Help us improve the DSL library, optimize MCTS strategies, or enhance the Value Net. ğŸ™Œ

## ğŸ“œ License ğŸ“„

MIT License
